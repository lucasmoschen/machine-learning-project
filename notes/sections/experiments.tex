\section{Experiment settings}
\label{sec:experiments}

We have already separated the dataset into training (70\%) and testing (30\%). In this
section we describe the methods used for the estimation: {\em Linear
Regression}, {\em Support Vector Machine}, {\em Random Forest}, and a Linear Regression that handle missing data simultaneously with the
fitting, using the {\em Expectation Maximization}. Each gas and each localization has its own model. 

\subsection{Linear Regression}

The first attempt was to consider linear regression. In this case, the model
supposes that the expected value of the gases quantity (conditioned on the
data) is an affine transformation of the independent variables. 

First we apply the simple OLS on the whole dataset. In general this is not a
great model, because it adds variance on the estimation and, for that reason, we consider a regularization term (elastic
net) with two parameters: $\alpha$ to control the penalty, and $w_{l1}$ to
control the weight given to $\mathcal{L}_1$ penalty. The parameters are chosen
with cross validation (5-Fold). This approach allows adding polynomial and
interaction terms, since the $\mathcal{L}_1$ penalty sets several parameters
to zero. The 5-Fold is realized with $w_{l1} \in \{0.1, 0.2, ..., 0.9, 1.0\}$
and $\alpha \in \{1,2,3,...,20\}$. 

Other interesting approach to do feature selection, rather than $\mathcal{L}_1$
penalty is Forward Feature Selector. It is slower than Elastic Net, but it
selects a best subset of features given that it adds     features in a greedy
fashion. The estimator chooses the best feature based on
cross-validation score (with R$^2$) and it stops if the improving in the score is lesser than 1e-4. 

\subsection{Support Vector Regression}

It is an extension of Support Vector Machine (SVM) algorithms for
regression. Given the more than quadratic complexity of the algorithm, this
does not scale for datasets with more than 10,000 samples, as suggested by Scikit-learn User Guide \cite{svr-function,
scikit-learn}. For that reason, we
suppose a linear kernel. The loss function considered is $\mathcal{L}_1$ loss,
with parameter $\epsilon$. A regularization parameter $C$ is also added to the
model, such that, $C$ is inversely proportional to the strength of the
regularization. All columns are scaled to have mean 0 and variance 1. 
The parameters are calibrated with cross validation (5-Fold), with 
$C \in \{10^{-3}, 10^{-2}, ..., 10^2\}$ and $\epsilon =
\{0.001, 0.01, 0.1, 0.2, 0.3\}$. The problem with that approach is that there
will be 150 fittings + 150 predictions. If we set the maximum number of
iterations to be 5000, the program does not converge, but each of the fitting
takes 1min, what is impractical. For that reason, this model is very
problematic. 

\subsection{Random Forest}

The random forest regressor is an extension of decision tree with $B$
bootstrap samples such that each split considers $m$ predictors, that is the root of the number of predictors. The parameter
$c$ measures the complexity parameter (minimal cost-complexity pruning).
The criterion to measure the quality of a split is MSE, in order to reduce
variance. The minimum number of samples required to split is the parameter
$s$. 


\subsection{Linear Regression + Expectation Maximization}

In this scenario, we follow the approach developed by \cite{rubin1977} and
demonstrated by \cite{missing-values-estimation}. This method supposes the
data comes from a normal distribution with mean $\mu_{y,X}$ and covariance matrix
$\Sigma_{y,X}$, including the dependent and independent variables. It uses the Expectation Maximization (EM) algorithm to estimate
these parameters, despite the missing data. With the normal parameters estimated, the following formula
allows the specification of the regressor parameters:
$$
\beta = (\mu_y - \Sigma_{y,X}\Sigma_X^{-1}\mu_X, \Sigma_{y,X}\Sigma_X^{-1})^T.
$$
A forward variable selection in the same terms as before is applied. The data
transformations \ref{sec:data-transform} are done without the missing data imputation. The problem with this approach is the absence of a predictive frame. Despite
estimating the parameters, the prediction is not straightforward since the
data has missing values. To handle this problem, the data with imputation (as
explained in Section \ref{sec:data-preprocessing}) served to predict values.

\subsection{Summary} 

Therefore the considered models and hyperparameters are the following:

\begin{enumerate}
    \item Simple linear regression: all predictors, no hyperparameter. 
    \item Elastic-net regression: all predictors, $\alpha$ measuring the
    penalty strength, and $w_{l1}$ the weight for $\mathcal{L}_1$ penalty. 
    \item Forward Feature Selection + Linear regression: The number of
    features to select as function of a threshold.
    \item Support Vector Regression: all predictors, $\epsilon$ measures the
    loss, and $C$ the regularization term. The variables are transformed
    between 0 and 1. 
    \item Random Forest: all predictors, $B$ bootstrap samples, $c$ is the
    complexity parameter, and $s$ is the minimum number of samples to split. 
    \item Linear regression + missing data imputation: no additional
    parameters. 
\end{enumerate}